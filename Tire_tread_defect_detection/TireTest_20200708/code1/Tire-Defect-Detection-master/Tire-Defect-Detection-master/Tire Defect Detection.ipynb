{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications import InceptionResNetV2\n",
    "from six.moves import cPickle\n",
    "# from inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import Flatten, Dense, AveragePooling2D,Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import rmsprop, SGD\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import to_categorical \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet():\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    # and a logistic layer -- 10 classes for Tire Defects\n",
    "    predictions = Dense(y_train_categorical.shape[1], activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy')\n",
    "    model.fit(x_train, y_train_categorical)\n",
    "    \n",
    "    layer_num = len(model.layers)\n",
    "    for layer in base_model.layers[:175]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[175:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train_categorical, epochs=5, shuffle=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reshape(data):\n",
    "    large_img = cv2.resize(img, dsize=(500, 700), interpolation=cv2.INTER_CUBIC)\n",
    "    return large_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL = 300\n",
    "def Datasetprep():\n",
    "    # Data preparation\n",
    "    path = './'\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for folder in os.listdir(path):\n",
    "        folderPath = path + folder\n",
    "        if folderPath[-3:] == '.py' or folderPath[-5:] == '.json':\n",
    "            continue\n",
    "        for image_name in os.listdir(folderPath):\n",
    "            img = cv2.imread(os.path.join(folderPath,image_name))\n",
    "            resizedImg = cv2.resize(img,(PIXEL,PIXEL))\n",
    "            x_train.append(np.array(resizedImg))\n",
    "            y_train.append(int(folder))\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train_categorical = to_categorical(y_train)\n",
    "    return x_train, y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 300, 300, 3) (186, 14)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train_categorical = Datasetprep()\n",
    "print(x_train.shape, y_train_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/1\n",
      "182/182 [==============================] - 74s 409ms/step - loss: 5.5483\n",
      "Train on 127 samples, validate on 55 samples\n",
      "Epoch 1/5\n",
      "127/127 [==============================] - 77s 610ms/step - loss: 1.7743 - acc: 0.5984 - val_loss: 1.2139 - val_acc: 0.7091\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 76s 595ms/step - loss: 0.9820 - acc: 0.7087 - val_loss: 1.6695 - val_acc: 0.7091\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 75s 593ms/step - loss: 1.3164 - acc: 0.6063 - val_loss: 1.2378 - val_acc: 0.6909\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 75s 592ms/step - loss: 0.8113 - acc: 0.7165 - val_loss: 1.5140 - val_acc: 0.7091\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 76s 598ms/step - loss: 0.9307 - acc: 0.7244 - val_loss: 1.6384 - val_acc: 0.6727\n"
     ]
    }
   ],
   "source": [
    "# For Training the Model\n",
    "# WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dimmodel, history = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Weights of the Model\n",
    "# model.save_weights('Resnt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Prediction(x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    print(show_history(history))\n",
    "    print(predictions)\n",
    "    print(x_train.shape,y_train_categorical.shape)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessingImage(image):\n",
    "    \n",
    "    test = []\n",
    "    resizedImage = cv2.resize(image, dsize=(PIXEL, PIXEL))\n",
    "    test.append(resizedImage)\n",
    "    test = np.array(test)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def json_write(idx,url1,url2,url3):\n",
    "    arr = [idx]\n",
    "    arrn = []\n",
    "    for i in range(0,13):\n",
    "        x = arr.__contains__(i)\n",
    "        if x==False:\n",
    "            arrn.append(i)\n",
    "\n",
    "    data = {}  \n",
    "    data['tyres'] = []  \n",
    "    data['num'] = []\n",
    "    data['url'] = []\n",
    "    data['url'].append({\n",
    "        'url1':url1,\n",
    "        'url2':url2,\n",
    "        'url3':url3\n",
    "    })\n",
    "    if(arr.__contains__(0)):\n",
    "            data['num'].append({  \n",
    "                'title': '0',\n",
    "                    })\n",
    "    for i in arr:\n",
    "        \n",
    "        '''if i == 0:\n",
    "                                        data['tyres'].append({  \n",
    "                                        'title': 'good',\n",
    "                                            })'''\n",
    "        if i == 1:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Tread wear indicator',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 2:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Off center snacking',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 3:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Bare/Lightness',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 4:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Blister',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 5:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Foreign Material',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 6:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Open cord',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 7:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Exposed Cord',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 8:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Crown deformation',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 9:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Bladder crease',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 10:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Separation',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 11:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Line Air',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 12:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Over Shaping',\n",
    "                'description': 'present'\n",
    "            })\n",
    "        if i == 13:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Scorch Rubber',\n",
    "                'description': 'present'\n",
    "            })\n",
    "    if(arrn.__contains__(0)):\n",
    "            data['num'].append({  \n",
    "                'title': '1',\n",
    "                    })\n",
    "    for i in arrn:\n",
    "        \n",
    "        if i == 1:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Tread wear indicator',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 2:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Off center snacking',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 3:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Bare/Lightness',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 4:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Blister',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 5:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Foreign Material',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 6:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Open cord',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 7:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Exposed Cord',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 8:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Crown deformation',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 9:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Bladder crease',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 10:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Separation',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 11:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Line Air',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 12:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Over Shaping',\n",
    "                'description': 'null'\n",
    "            })\n",
    "        if i == 13:\n",
    "            data['tyres'].append({  \n",
    "                'title': 'Scorch Rubber',\n",
    "                'description': 'null'\n",
    "            })\n",
    "    \n",
    "\n",
    "    with open('data.json', 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "\n",
    "# arr = [idx]\n",
    "# arrn = []\n",
    "# for i in range(0,13):\n",
    "#     x = arr.__contains__(i)\n",
    "#     if x==False:\n",
    "#         arrn.append(i)\n",
    "# print(idx)\n",
    "# print(arrn)\n",
    "\n",
    "#path = os.path.dirname(__file__)      \n",
    "#url1 = \"http://bitsotg.com:7878\" +\"/image1.jpg\"\n",
    "#url2 = \"http://bitsotg.com:7878\" +\"/image4.jpg\"\n",
    "#url3 = \"http://bitsotg.com:7878\" +\"/image7.jpg\"\n",
    "# print(url)\n",
    "#json_write(arr,arrn,url,url,url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = url2 = url3 = \"\"\n",
    "def Predict(IMG_PATH, model):\n",
    "    test_img = cv2.imread(IMG_PATH)\n",
    "    \n",
    "    if test_img is None or test_img.shape[0] == 0:\n",
    "        return -1\n",
    "    cv2.imshow('Test Image', test_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    url = url1 = url2 = url3 = \"http://192.168.43.196:7879/\"+ IMG_PATH[2:]\n",
    "    print(url1)\n",
    "    processed_img = ProcessingImage(test_img)\n",
    "    predictions_resnet = model.predict(processed_img)\n",
    "    print(predictions_resnet)\n",
    "    idx = FindingIndex(predictions_resnet)\n",
    "    json_write(idx,url,url,url)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindingIndex(predictions_resnet):\n",
    "    idx = -1\n",
    "    idx2 = -1\n",
    "    idx3 = -1\n",
    "    maxVal = 0\n",
    "    secondLargest = 0\n",
    "    ThirdLargest = 0\n",
    "    k = 0\n",
    "    for i in range(predictions_resnet.shape[0]):\n",
    "        for j in range(predictions_resnet.shape[1]):\n",
    "\n",
    "            if predictions_resnet[i][j] > maxVal:\n",
    "                idx = k\n",
    "                ThirdLargest = secondLargest\n",
    "                secondLargest = maxVal\n",
    "                maxVal = predictions_resnet[i][j]\n",
    "            elif predictions_resnet[i][j] > secondLargest:\n",
    "                ThirdLargest = secondLargest\n",
    "                secondLargest = predictions_resnet[i][j]\n",
    "                idx2 = k\n",
    "            elif predictions_resnet[i][j] > ThirdLargest:\n",
    "                ThirdLargest = predictions_resnet[i][j]\n",
    "                idx3 = k\n",
    "            k += 1\n",
    "    print(\"First Largest index Prediction is \" + str(idx)+\" With Prediction \"+ str(maxVal))\n",
    "    print(\"Second Closest index Prediction is \" + str(idx2)+\" With Prediction \"+ str(secondLargest))\n",
    "    print(\"Third Largest index Prediction is \" + str(idx3)+\" With Prediction \"+ str(ThirdLargest))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "186/186 [==============================] - 79s 427ms/step - loss: 4.8657\n",
      "Train on 130 samples, validate on 56 samples\n",
      "Epoch 1/5\n",
      "130/130 [==============================] - 79s 608ms/step - loss: 1.9061 - acc: 0.5769 - val_loss: 0.9396 - val_acc: 0.7143\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 80s 616ms/step - loss: 1.1362 - acc: 0.6000 - val_loss: 1.2730 - val_acc: 0.6964\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 81s 621ms/step - loss: 0.8399 - acc: 0.7692 - val_loss: 1.5124 - val_acc: 0.6607\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 77s 593ms/step - loss: 0.6813 - acc: 0.7769 - val_loss: 1.9118 - val_acc: 0.6964\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 83s 635ms/step - loss: 0.7082 - acc: 0.7692 - val_loss: 1.5235 - val_acc: 0.6964\n"
     ]
    }
   ],
   "source": [
    "''' Some Sample Paths for testing the Prediction of the Model\n",
    "    IMG_PATH = \"./Images/4/bmSwAtC.png\"\n",
    "    IMG_PATH = \"./Images/13/JFPpxUg.png\"\n",
    "    IMG_PATH = \"./Images/0/03SINCERA-845.png\"\n",
    "    IMG_PATH= \"./Images/11/UbQ02HU.png\"\n",
    "    IMG_PATH = \"./Images/0/12x.png\"\n",
    "    IMG_PATH = \"./Images/3/download (2).jpg\"\n",
    "    IMG_PATH = \"./Images/12/WhatsApp Image 2019-03-03 at 2.33.42 AM.jpeg\"\n",
    "    IMG_PATH = \"./Images/8/Bulge-on-Tire-Sidewall.png\"\n",
    "    IMG_PATH = \"./Images/12/images.jpg\" '''\n",
    "# path = IMG_PATH\n",
    "# Model Binded Together\n",
    "x_train, y_train_categorical = Datasetprep()\n",
    "model, history = ResNet()\n",
    "# model.save_weights('Resnet_weights.h5')\n",
    "\n",
    "#Saving the Prev Model\n",
    "# saveModel(model)\n",
    "IMG_PATH = \"./Images/0/03SINCERA-845.png\"\n",
    "idx = Predict(IMG_PATH,model)\n",
    "# print(url)\n",
    "#Reading the Existing Model\n",
    "# new_model = ReadModel()\n",
    "# print(\"New One\")\n",
    "# print(Predict(IMG_PATH, new_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model):\n",
    "    model.save('my_model.h5')\n",
    "    return\n",
    "\n",
    "def ReadModel():\n",
    "    \n",
    "    model = load_model('my_model.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12]\n",
      "http://192.168.43.196:7879/Images/8/Bulge-on-Tire-Sidewall.png\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
